{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hu7DJGOB1qgC"
      },
      "source": [
        "# Лабораторна робота №5\n",
        "**Тема:** Ансамблеві методи\n",
        "\n",
        "## Мета роботи\n",
        "Закріпити поняття про основні ансамблеві методи в машинному навчанні.\n",
        "\n",
        "## Теоретичні відомості\n",
        "Ансамблеві методи — це підхід у машинному навчанні, який поєднує кілька моделей для отримання більш точного та стійкого результату.\n",
        "\n",
        "Основні ансамблеві методи:\n",
        "\n",
        "- **Bagging (Bootstrap Aggregating)** — зменшує варіативність моделі за рахунок навчання на різних підвибірках даних.\n",
        "\n",
        "- **Random Forest** — ансамбль дерев рішень, що використовує випадковий вибір ознак.\n",
        "\n",
        "- **Boosting** — послідовне навчання слабких моделей, де кожна наступна фокусується на помилках попередніх.\n",
        "\n",
        "- **Stacking** — комбінування різних моделей через мета-модель.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Крок 1.** Імпортуйте необхідні бібліотеки"
      ],
      "metadata": {
        "id": "ghTo0KdycQjt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-th4g9M1qgF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Крок 2**. Завантажте дані. Візьмемо Breast Cancer dataset зі sklearn.datasets (це реальні медичні дані про пухлини молочної залози).\n",
        "\n",
        "Кількість об’єктів (рядків): 569 пацієнтів.\n",
        "\n",
        "Кількість ознак (стовпців): 30 числових характеристик, отриманих із медичних зображень клітин.\n",
        "\n",
        "Цільова змінна (y):\n",
        "\n",
        "0 — злоякісна пухлина (malignant)\n",
        "\n",
        "1 — доброякісна пухлина (benign)\n",
        "\n",
        "Основні ознаки\n",
        "Кожна ознака описує певні характеристики клітин, наприклад:\n",
        "\n",
        "mean radius — середній радіус клітини\n",
        "\n",
        "mean texture — текстура клітини\n",
        "\n",
        "mean perimeter — середній периметр\n",
        "\n",
        "mean area — площа клітини\n",
        "\n",
        "mean smoothness — гладкість поверхні\n",
        "\n",
        "mean compactness — компактність\n",
        "\n",
        "mean concavity — увігнутість\n",
        "\n",
        "mean symmetry — симетрія\n",
        "\n",
        "fractal dimension — фрактальна розмірність\n",
        "\n",
        "Для кожної характеристики є кілька статистичних показників:\n",
        "\n",
        "mean (середнє значення)\n",
        "\n",
        "se (standard error — стандартна похибка)\n",
        "\n",
        "worst (найгірше значення серед усіх клітин пацієнта)"
      ],
      "metadata": {
        "id": "MviPVeXdheht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Завантажуємо набір даних\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target)\n",
        "\n",
        "print(\"Розмірність X:\", X.shape)\n",
        "print(\"Класи:\", np.unique(y))"
      ],
      "metadata": {
        "id": "lDPAsPYnirS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Крок 3. Розділення на тренувальну та тестову вибірки**\n",
        "\n"
      ],
      "metadata": {
        "id": "nsAQoIiquCeG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")"
      ],
      "metadata": {
        "id": "CHjYxzBPuOnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Крок 4.** Навчання моделей ансамблів\n",
        "\n",
        "У цьому кроці ми навчаємо три популярні ансамблі: Bagging, Random Forest та Gradient Boosting. Кожен підхід по-різному бореться з похибками моделі: зменшує варіативність (variance), знижує зміщення (bias) або робить обидва одночасно. Нижче — детально про кожну модель, ключові параметри та пояснення коду."
      ],
      "metadata": {
        "id": "zSIQ96PzinUU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bagging: зменшення варіативності через бутстрепінг**\n",
        "\n",
        "Ідея: навчити багато “слабких” базових моделей на різних випадкових підвибірках даних та усереднити їх передбачення.\n",
        "\n",
        "Ключові параметри:\n",
        "\n",
        "n_estimators: більше — стабільніше, але довше навчається.\n",
        "\n",
        "bootstrap: увімкнений — кожна модель бачить трохи іншу вибірку (з повторенням).\n",
        "\n",
        "max_samples / max_features: додають випадковість і зменшують кореляцію між моделями.\n",
        "\n",
        "Що відбувається під час fit: для кожної базової моделі генеруються свої бутстреп-дані та підмножина ознак; потім усі моделі навчаються паралельно, а на етапі predict їхні голоси агрегуються (мода для класифікації).\n",
        "\n",
        "Коли корисно: при схильності базового алгоритму до перенавчання і високої варіативності (наприклад, дерева рішень)."
      ],
      "metadata": {
        "id": "844Y9DITFpbf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "bagging = BaggingClassifier(\n",
        "    n_estimators=50,        # кількість базових моделей\n",
        "    max_samples=1.0,        # частка зразків у кожному бутстрепі\n",
        "    max_features=1.0,       # частка ознак, які беруться в кожній моделі\n",
        "    bootstrap=True,         # вибірка з поверненням (бутстреп)\n",
        "    random_state=42         # фіксує випадковість для відтворюваності\n",
        ")\n",
        "bagging.fit(X_train, y_train)\n",
        "y_pred_bagging = bagging.predict(X_test)\n"
      ],
      "metadata": {
        "id": "pePz6DuiiopD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Random Forest: баггінг дерев + випадковий вибір ознак**\n",
        "\n",
        "Ідея: це спеціальний випадок баггінгу, де базові моделі — дерева рішень, а на кожному спліті дерево розглядає випадкову підмножину ознак. Це зменшує кореляцію дерев і покращує узагальнення.\n",
        "\n",
        "Ключові параметри:\n",
        "\n",
        "max_features=\"sqrt\": для класифікації типове значення; балансує різноманітність і силу дерев.\n",
        "\n",
        "max_depth, min_samples_split, min_samples_leaf: контролюють складність дерев і ризик перенавчання.\n",
        "\n",
        "n_estimators: більше дерев — краща стабільність, але довший час.\n",
        "\n",
        "Інтерпретація: можна подивитися важливість ознак (feature_importances_) після навчання, щоб зрозуміти внесок кожної ознаки.\n",
        "\n",
        "Коли корисно: коли багато ознак, є нелінійні взаємодії, і потрібна надійна базова точність без тонкого тюнінгу.\n",
        "\n"
      ],
      "metadata": {
        "id": "gj1rvGxfKO-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=100,       # кількість дерев\n",
        "    max_depth=None,         # необмежена глибина (можна регулювати)\n",
        "    max_features=\"sqrt\",    # випадковий підбір ознак на кожному спліті\n",
        "    min_samples_split=2,    # мінімум зразків для поділу\n",
        "    min_samples_leaf=1,     # мінімальний розмір листка\n",
        "    bootstrap=True,         # кожне дерево — на бутстреп-даних\n",
        "    n_jobs=-1,              # використати всі ядра CPU\n",
        "    random_state=42\n",
        ")\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred_rf = rf.predict(X_test)"
      ],
      "metadata": {
        "id": "zXqjNz1oGI-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gradient Boosting: послідовне навчання зі зменшенням помилок**\n",
        "\n",
        "Ідея: будуємо модель поетапно. Кожне наступне “слабке” дерево навчається виправляти помилки попереднього ансамблю, рухаючись у напрямку антиградієнта функції втрат.\n",
        "\n",
        "Ключові параметри:\n",
        "\n",
        "learning_rate: менший — обережніший крок, зазвичай вимагає більше дерев; часто дає кращу узагальнювальну здатність.\n",
        "\n",
        "n_estimators: кількість етапів; збільшує потужність моделі, але може сприяти перенавчанню.\n",
        "\n",
        "max_depth: тримає базові дерева слабкими, що стабілізує бустинг.\n",
        "\n",
        "subsample < 1.0: вмикає стохастичний бустинг (часткова вибірка), що додає регуляризацію і може покращити якість.\n",
        "\n",
        "Коли корисно: для високої точності та тонкого контролю; чутливий до параметрів, але часто дає найкращі результати."
      ],
      "metadata": {
        "id": "110L31Y4Gulv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "gb = GradientBoostingClassifier(\n",
        "    n_estimators=100,       # кількість кроків бустингу (дерев)\n",
        "    learning_rate=0.1,      # вага кожного нового дерева\n",
        "    max_depth=3,            # глибина базового дерева (слабкий учень)\n",
        "    subsample=1.0,          # частка зразків для кожного кроку (1.0 — без стохастики)\n",
        "    random_state=42\n",
        ")\n",
        "gb.fit(X_train, y_train)\n",
        "y_pred_gb = gb.predict(X_test)"
      ],
      "metadata": {
        "id": "YqPIf1Y-G6YO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Крок 5. Аналіз результатів**\n",
        "\n",
        "Аналіз результатів\n",
        "Після виконання кроку 5 ми отримали точність (accuracy) для трьох моделей:\n",
        "\n",
        "Bagging — зазвичай показує стабільну точність, зменшує варіативність.\n",
        "\n",
        "Random Forest — часто має найвищу точність, бо поєднує баггінг і випадковий вибір ознак.\n",
        "\n",
        "Gradient Boosting — може дати ще кращі результати, але чутливий до параметрів (learning_rate, n_estimators).\n",
        "\n",
        "Крім accuracy, важливо дивитися на:\n",
        "\n",
        "Precision (точність) — частка правильних позитивних передбачень серед усіх позитивних.\n",
        "\n",
        "Recall (повнота) — частка знайдених позитивних випадків серед усіх реальних позитивних.\n",
        "\n",
        "F1-score — баланс між precision та recall.\n",
        "\n",
        "Для медичних даних (як у нашому випадку) Recall особливо важливий, бо пропустити злоякісну пухлину небезпечніше, ніж помилково класифікувати доброякісну.\n"
      ],
      "metadata": {
        "id": "IZCpOBhUxLN2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, classification_report, roc_curve, auc\n",
        "\n",
        "# --- Обчислення точності ---\n",
        "results = {\n",
        "    \"Bagging\": accuracy_score(y_test, y_pred_bagging),\n",
        "    \"Random Forest\": accuracy_score(y_test, y_pred_rf),\n",
        "    \"Gradient Boosting\": accuracy_score(y_test, y_pred_gb)\n",
        "}\n",
        "\n",
        "# --- Вивід точності та звіту ---\n",
        "print(\"Bagging Accuracy:\", results[\"Bagging\"])\n",
        "print(\"Random Forest Accuracy:\", results[\"Random Forest\"])\n",
        "print(\"Gradient Boosting Accuracy:\", results[\"Gradient Boosting\"])\n",
        "\n",
        "print(\"\\nЗвіт для Random Forest:\\n\", classification_report(y_test, y_pred_rf))"
      ],
      "metadata": {
        "id": "dgSeV1ctkgkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Крок 6** Візуалізація\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nnM507PPxzGg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.bar(results.keys(), results.values(), color=['skyblue','green','orange'])\n",
        "plt.title(\"Порівняння точності ансамблевих методів\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.ylim(0.9, 1.0)  # бо точність дуже висока\n",
        "for i, v in enumerate(results.values()):\n",
        "    plt.text(i, v+0.002, f\"{v:.3f}\", ha='center', fontsize=10)\n",
        "plt.show()\n",
        "\n",
        "# --- ROC-криві для кожної моделі ---\n",
        "plt.figure(figsize=(8,6))\n",
        "\n",
        "# Bagging\n",
        "fpr_bag, tpr_bag, _ = roc_curve(y_test, bagging.predict_proba(X_test)[:,1])\n",
        "roc_auc_bag = auc(fpr_bag, tpr_bag)\n",
        "plt.plot(fpr_bag, tpr_bag, label=f'Bagging (AUC = {roc_auc_bag:.3f})')\n",
        "\n",
        "# Random Forest\n",
        "fpr_rf, tpr_rf, _ = roc_curve(y_test, rf.predict_proba(X_test)[:,1])\n",
        "roc_auc_rf = auc(fpr_rf, tpr_rf)\n",
        "plt.plot(fpr_rf, tpr_rf, label=f'Random Forest (AUC = {roc_auc_rf:.3f})')\n",
        "\n",
        "# Gradient Boosting\n",
        "fpr_gb, tpr_gb, _ = roc_curve(y_test, gb.predict_proba(X_test)[:,1])\n",
        "roc_auc_gb = auc(fpr_gb, tpr_gb)\n",
        "plt.plot(fpr_gb, tpr_gb, label=f'Gradient Boosting (AUC = {roc_auc_gb:.3f})')\n",
        "\n",
        "# --- Оформлення графіка ---\n",
        "plt.plot([0,1],[0,1],'k--')  # лінія випадкової класифікації\n",
        "plt.title(\"ROC-криві ансамблевих методів\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZLgUGcIIMqrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vS1Krr8UiWZN"
      },
      "source": [
        "**Висновки**\n",
        "\n",
        "Усі три ансамблеві методи показують високу якість класифікації на реальних даних.\n",
        "\n",
        "Random Forest часто є \"золотою серединою\": простий у налаштуванні, швидкий і точний.\n",
        "\n",
        "Gradient Boosting може перевершити Random Forest, але потребує тонкого налаштування параметрів.\n",
        "\n",
        "Bagging добре працює як базовий метод для зменшення варіативності."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Завдання для самостійного виконання**\n",
        "\n",
        "Завантажити інший набір даних зі sklearn.datasets (наприклад, Iris, Wine, або Digits) чи з відкритих джерел (Kaggle, UCI).\n",
        "\n",
        "Побудувати ансамблеві моделі (Bagging, Random Forest, Gradient Boosting).\n",
        "\n",
        "Порівняти їхню точність та зробити висновки."
      ],
      "metadata": {
        "id": "2eAbmWklg45G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Тут має бути Ваш код\n"
      ],
      "metadata": {
        "id": "qVm35MLcnozM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Тут мають бути Ваші висновки"
      ],
      "metadata": {
        "id": "LbSguIBChruX"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "generative_ai_disabled": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}